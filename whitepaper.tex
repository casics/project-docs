\documentclass{casicswhitepaper}
\usepackage{bchart}
\renewcommand{\bcfontstyle}{}

\newcommand{\totalrespondents}{69\xspace}
\newcommand{\totaldevelopers}{56\xspace}
\newcommand{\totalsearchers}{55\xspace}

\begin{document}
\title{An empirical study of approaches used by scientific users to find software}
\date{\today}
\author{Matthew J. Graham and Michael Hucka\\
\mailto{mjg@caltech.edu},\quad\mailto{mhucka@caltech.edu}\\
California Institute of Technology\\
Pasadena, CA 91125, USA}
\maketitle

\begin{abstract}
When they seek software for a task, how do scientists go about it?  Past research and anecdotal evidence suggest that searching the web, asking colleagues and reading papers have been the primary approaches used---but is that still true today, given the increasingly greater use of social media and socially-centric online systems such as StackOverflow and GitHub?  In addition, when they \emph{do} look for software, what are some of the main characteristics that influence scientific users' choice of software?  And finally, if a systematic catalog of software were to be developed, what kind of information would users like to see in it?  These are the questions that motivated a survey we conducted in late 2015.  The survey was designed to capture current practices and experiences in searching for software from two distinct groups: those looking for ready-to-run software and those looking for source code to incorporate into their own code base respectively.  We present the results of our survey and discuss our findings in this report.
\end{abstract}


\section{Introduction}

Despite that software is integral to most research~\citep{bauer2014exploratory, hettrick_2014, hannay_2009, baxter_2006, wilson_2006}, finding software suitable for a given purpose remains surprisingly difficult~\cite{cannata_2005, Bourne::2015, SoftwareDiscoveryIndex:2014}.  There are few effective resources to help users discover alternatives or understand the differences between them.  When asked, many people reply that they search the web using a general-purpose search engine such as Google.  At first this approach seems obvious, but it suffers from a significant problem: web searches can yield dozens or hundreds of viable candidates---and \emph{millions} or irrelevant results.  Moreover, some questions cannot be answered through search without substantial effort, such as \emph{how many} software tools with certain capabilities exist, what are their \emph{specific} characteristics, or how different tools \emph{differ} from each other.  The difficulty of using search to find software was made clear in a recent paper in which the authors studied software developers at Google, Inc.  They found that the factor ``most disruptive to the [software] reuse process'' was ``difficulties in finding artifacts''~\cite{bauer2014exploratory}.  In other words, \emph{even the developers at Google have difficulty finding software}.

Numerous other methods are possible besides direct search for finding software.  For example, informally, we know that many scientists turn to the scientific literature to learn what others have used for similar tasks or research domains.  Searching the literature can increase the number of relevant results as well as provide other useful information, but it suffers from limitations too: publications can take months or years to produce and may not reflect a tool's \emph{current} capabilities~\cite{wren_2004}, and more importantly, \emph{not all useful software tools have a publication associated with them}.  Still other potential methods for finding software include asking other people, asking on social media, following institutional guidelines, and more.

The difficulty of finding software and the lack of better resources brings the potential for duplication of work, reduced scientific reproducibility, and poor return on investment by funding agencies~\citep{cannata_2005}.  We are interested in developing better resources to help users, particularly scientific users, discover software.  In order to gain a better understanding of the factors that influence how software users locate software, we developed and distributed an electronic survey in September, 2015, which we advertised to numerous mailing lists serving communities in astronomy and systems biology.  In total, we received \totalrespondents individual responses.  We report on our methods, the survey responses, and our analyses below.


\section{Survey structure}

Our survey was designed to capture current practices and experiences in searching for software from two distinct groups: those looking for ready-to-run software, and those looking for source code to incorporate into their own code base respectively.  We chose to use a Web-based survey because it is an approach that (1) is well-suited to gathering information quickly from a wide audience, (2) requires modest development effort, and (3) can produce data that can be analyzed qualitatively and quantitatively.


\subsection{Instrument development}

We developed the survey instrument iteratively.  We began with an initial version asking many questions related to searching for software.  Following the practices of other similar surveys in computing~\cite[e.g.,][]{varnellsarjeant2015comparing}, we iterated on the design of the instrument, paying attention to the following points: 

\begin{itemize}

\item Wording.  We sought to make the questions clear and unambiguous, and avoid implying an a particular perspective.  We elaborated each question with explanatory text under the question itself.

\item Relevance to user's experiences.  We limited our questions to topics that could reasonably be assumed to be within the experiences of our audience.

\item Contemporary circumstances.  We tried to ground the questions by referring to real resources and specific software characteristics that we believe are relevant to computer users today.

\item Ethics.  We avoided questions that might be construed as being too personal or about proprietary policies at respondents' place of work.

\end{itemize}

To help iterate on the design of the survey instrument, we performed a pilot survey with close colleagues as subjects.  Based on their feedback, we removed or expanded questions as necessary to achieve the final version.  The final survey form is presented in Appendix~A.  The instrument contained a total of 22 questions (of which 18 were content questions), with switching logic so that the final number of questions actually seen by a user depended on some of their responses. There were five main sections in the survey:

\begin{enumerate}

\item Basic demographic and general information, suitable for all respondents.

\item Questions for software users who have the freedom to choose software.  This section was only shown if respondents indicated that they have some choice in the software they use.

\item Questions for software developers.  This section was only shown if respondents indicated that are engaged in software development.

\item Questions for software developers who search for source code.  This was only shown if respondents indicated both that they are software developers and that they search for software source code.

\item Survey feedback.  This section sought feedback about the survey itself.

\end{enumerate}

Questions in section No.~2 aimed to establish the relative importance of different search criteria.  Those in section Nos.~3 and~4 sought to characterize the experiences of the developer.


\subsection{Sampling plan}

We used nonprobabilistic convenience sampling with self-selection.  We advertised the survey on mailing lists and social media oriented to the astronomical and biological sciences, particularly to computational subcommunities within those domains.    The number of potential participants is unknown, because we cannot track the redistribution of the survey invitation.  Thus, the response rate is unknown.

Potential biasing factors in the results are the same as those typical of all self-selected written surveys that use convenience sampling.  This include response bias (i.e., people who respond may have different characteristics than those who do not), coverage errors (i.e., the representation of respondents may not be balanced across different subcommunities), and item bias (i.e., some questions may have been skipped intentionally or unintentionally).  An additional possible source of bias is that we (the authors) are relatively well-known within the subcommunities to which we advertised the survey, and thus the respondents may have been influenced by their personal knowledge or relationships to us.  


\subsection{Administration}

We used convenience sampling with self-selection.  We advertised the survey on mailing lists and social media oriented to the astronomical and biological sciences, particularly to computational subcommunities within those domains.  Any recipients were free to participate if they chose.  The survey had no express closing date.  We analyzed the results obtained by December 31, 2015.


\section{Results: demographics}
\label{demographics}

One of the first questions in the survey was ``What is your primary field of work?'', with multiple choices and ``Other'' as the answer options.  \fig{disciplines} shows the answer choices and the number of responses.  Of \totalrespondents respondents, 57\% identified as working in the physical sciences, 46\% in computing and maths, 28\% in biological sciences and 7\% in a range of others.  Multiple areas of work were allowed, and some respondents indicated multiple disciplines.

% In the category of "other":
%
% - One person answered thus: Computing & Math Sci. (CMS), "astrophysics", "IT".  I added 1 to the count of physical sciences (the rationale being that astrophysics is a physical science) but didn't change the count of CMS because IT can be considered part of CMS at our level of granularity.
%
% - One person answered CMS, and "Pri. A&A" and "Cosmology".  I added 1 more to physical sciences for the A&A and cosmology (because they did not also select Physical Sciences, so this is a case where the count needs to be adjusted).
%
% - One person answered "Data science" but not also CMS, so I added 1 to CMS.
%
% - Two people answered "Astronomy" for "other" as well as answering "physical sciences".  We would lump astronomy under physical sciences, so it does not change the count for physical sciences.
%
% - One person answered "Aerospace Engineering (Robotics)".  I don't know where to put that one, so left it as a real "Other".

\begin{figure}[htb]
  \vspace*{-1ex}
  \centering
  \includegraphics{files/plots/respondents-by-discipline.pdf}
  \vspace*{-2ex}
  \caption{Respondents by discipline.  The survey offered the first eight predefined categories and an additional slot for free text under ``Other''.  Choices were non-exclusive.  In some cases, respondents provided values for ``Other'' but the values were actually subsumed by one of the predefined categories; in those cases, we adjusted the totals appropriately.  One response, ``Aerospace Engineering (Robotics)'', did not fit any predefined category; we included it as a true ``Other'' value.}
  \label{disciplines}
\end{figure}

\begin{figure}[b]
  \centering
  \includegraphics[trim=0.1in 0.1in 0 0,width=6.25in]{files/plots/bar-graph-time-spent-with-software.pdf}
  \caption{Bar graph of responses to the question ``In your work, on a
    typical day, approximately what fraction of your time involves using or
    interacting directly with software on a computer or other computing
    device?''}
  \label{time-with-software}
\end{figure}

We also asked ``In your work, on a typical day, approximately what fraction of your time involves using or interacting directly with software on a computer or other computing device?''  The answer choices were provided in the form of a pull-down menu with values ranging from 0\% (none) to 100\% (all), in 5\% increments.  \fig{time-with-software} provides a bar graph of the responses.  Assuming a typical 8 hour working day, we can conclude that 94\% of respondents regularly spent more than four hours of their time engaged with software, and 68\% more than six hours.

In another question, we asked, ``In your work, how much freedom do you usually have to choose the software you use?''.  \fig{freedom} provide the results.  All in all,  88\% of respondents said that they were free to make software choice decisions.

\begin{figure}[thb]
  \centering
  \includegraphics[trim=0.1in 0.1in 0 0,width=6.1in]{files/plots/how-often-choose-software.pdf}
  \vspace*{-4ex}
  \caption{Responses to the question ``In your work, how much freedom do you usually have to choose the software you use?''.  (Note: the full text of the response option ``Half the time'' was ``Half the time, a situation or task requires using preselected software, and other times I get to choose''.)}
  \label{freedom}
\end{figure}

We also asked ``Are you involved in software development?''.  In all, \totaldevelopers (81\%) answered ``Yes'' and 13 (19\%) answered ``No''.  The answer to this question was used to control the availability of other questions in the survey form.  Among the questions that were made available for those who answered ``Yes'' were additional demographic questions.  The first such question asked ``For how many years have you been developing software?'' with a free-form text field for answers.  \fig{years} provides a histogram of the responses received.

% Note about the data cleaning:
% - if someone wrote "x+" or "at least x", I counted it as x.
% - if someone wrote "~x", I counted it as x.
% - one person wrote "depends on who you ask". I deleted that result.

\begin{figure}[thb]
  \hspace*{0.5in}
  \begin{minipage}[b]{0.25\linewidth}
    \includegraphics[width=1.35in]{files/plots/number-of-developers.pdf}
  \end{minipage}%
  \begin{minipage}[b]{0.75\linewidth}
    \includegraphics[width=3.5in]{files/plots/histogram-years.pdf}
  \end{minipage}%
  \vspace*{-2ex}
  \caption{(Left) Responses to the question  ``Are you involved in software development?'' (Right) Histogram of years that respondents have been developing software (for those who also answered ``Yes'' to the question of whether they were involved in software development).}
  \label{years}
\end{figure}

Another question asked of those who indicated they were involved in software development was ``In your current (or most recent) software development project, what is (or was) your primary responsibility?'' The answers were in the form of eight multiple choice items and a ninth ``Other'' choice with a free-form text field.  The choices were non-exclusive: although we asked for people's primary responsibility, respondents were free to choose more than one and the explanatory text for the question indicated ``If it is hard to identify a single one, you can indicate more than one below.''  \fig{responsibilities} provides a tally of the responses.

\begin{figure}[thb]
  \vspace*{-1ex}
  \centering
  \includegraphics[width=6in]{files/plots/responsibilities.pdf}
  \vspace*{-5ex}
  \caption{Responsibilities indicated by the 56 respondents who answered ``Yes'' to the question of whether they were involved in software development.  This survey question offered the first eight predefined categories and an additional slot for free text under ``Other''; only one respondent provide a value for ``Other''.  Choices were non-exclusive.}
  \label{responsibilities}
\end{figure}

In another question, we then asked, ``What is the typical team size of projects you are involved with?''  The answers were again in the form of multiple choices with an ``Other'' choice that offered a free-form text field.  Answers were provided by all \totaldevelopers respondents who answered ``Yes'' to the question of whether they were involved in software development, and none of the respondents selected ``Other''.  \fig{project-sizes} provides a summary of the results.

\begin{figure}[thb]
  \centering
  \includegraphics{files/plots/project-sizes.pdf}
  \vspace*{-2ex}
  \caption{Project sizes indicated by the \totaldevelopers respondents who answered ``Yes'' to the question of whether they were involved in software development.}
  \label{project-sizes}
\end{figure}

Finally, we asked ``Which programming and/or scripting language(s) have you had the most experience with?''  This question provided 22 predefined language choices as multiple choices along with a free-text ``Other'' option.  Choices were non-exclusive, and the elaboration under the question explicitly requested ``Please select up to 3 languages which you have used the most''.   The top five responses were: Python (selected by 59\% of respondents), C (50\%), Java (34\%), shell scripting (32\%), and C++ (27\%).

These responses are consistent with most expectations for the targeted scientific communities.  We expected to reach computer literate individuals, and due to the distribution channels we used, most likely reached those working in research environments.  Most respondents indicated they are involved in software development, and typical development team sizes were small, with 77\% being in groups of 1 to 5 persons.  This is common in scientific software development, and the fact that many respondents indicated they had multiple roles is also consistent---small teams generally require members to take on more than one role.

Amongst the 81\% of respondents who subsequently indicated that they were involved in software development to some degree (and not just end users), the median number of years of software development experience was 20. This also suggests that the typical respondent is mid-career or part of the pre-mobile device computing generation. In addition to software development per se, 65\% indicated that they were also primarily responsible for project management or software architecture, which are traditionally more senior roles.  The demographic data may thus indicate a possible bias in responses against more junior members of the respective communities, such as students and postdocs.  This is of concern because junior members may have different search criteria and development experiences than their more experienced colleagues. This possibility should be borne in mind when interpreting the survey results. The cause of this distribution is unknown.  We speculate that it may be the result of a degree of self selection, in that more experienced individuals are more likely to participate in community surveys.  In any case, the possible experience bias is something that we should aim to redress in future similar efforts.


\section{Results: how respondents find software}

Numbers in parentheses in this section indicate the relative rankings or levels of importance (essential and above-average) given in the survey.


\subsection{Ready-to-use software}
 
We first consider the search for software to use for a particular task rather for development purposes. This does not necessarily imply searching for source code, and our questions emphasized that ``ready-to-run'' was the primary goal.

 
\subsubsection{Approaches}

To assess how people located or discovered ready-to-run software, we asked the multiple-choice question ``When you need to find ready-to-run software for a particular task, how do you go about finding software?''  The question provided many answer options together with a free-text ``Other'' option.  The specific, predefined answer options were constructed based on our own experiences as well as the questions posed in other surveys~\cite{sim_2011, bajrachary_2009, linstead_2009} and the results of the pilot runs of our survey form.  Respondents were free to choose more than one answer.  \fig{how-find-ready-to-run} summarizes the results.

\begin{figure}[thb]
  \centering
  \includegraphics[trim=0.1in 0.1in 0 0,width=6in]{files/plots/how-find-ready-to-run.pdf}
  \vspace*{-4ex}
  \caption{Responses to the question ``When you need to find ready-to-run software for a particular task, how do you go about finding software?'' Answer choices were non-exclusive. All \totalrespondents survey participants answered the question.}
  \label{how-find-ready-to-run}
\end{figure}

Personal recommendations (83\%) and general search engines (90\%) are the two main mechanisms employed to find software by our respondents. If a similar task is also described in the literature, then this will be used as a guide (62\%) to identify relevant software to use. Online forums, whether general social media sites or more focused in scope, are an underutilized resources (mean of 21\%), although people are slightly more likely to search in a public software project repository (32\%) but not in a domain specific one (10\%). This latter usage pattern may reflect ignorance of the existence of the topical software indexes in question, but may also reflect a belief that such resources are too narrowly focused in scope for their needs. Unfortunately, the question did not address the size of the task being searched for so we cannot answer this for sure.

The write-in answers for ``Other'' revealed also a category of options we did not anticipate: all of the answers concerned the use of network-based software package installation systems such as MacPorts~\cite{fuller2002macports} and the systems available for the different Linux operating system distributions.  In retrospect, this is an obvious oversight in our list of predefined categories---the package management systems offer search capabilities, and thus, this is indeed another way for a person to find ready-to-run software.  Future versions of this survey should include this as a predefined answer choice.


\subsubsection{Criteria}

We sought to understand the different selection and evaluation criteria that may come into play when users try to find ready-to-run software.  To this end, we posed the question ``In general, how important are the following characteristics when you are searching for ready-to-run software for a task?''  For the answer choices, we provided a two-dimensional grid with different predefined criteria as the rows and, as the columns, check boxes for rating each criterion on an importance scale.  The values on the scale were ``Rarely or never important'', ``Somewhat or occasionally important'', ``Average importance'', ``Usually of above-average importance'', and ``Essential''. \fig{criteria-ready-to-run} summarizes the results.  Analysis of the answers revealed that not all respondents provided a value for every criterion in the table, which may be due either to oversight (e.g., if they did not notice they missed a row in the grid) or confusion about the instructions (if they thought they should only rate the ones they cared about).

\begin{figure}[thb]
  \centering
  \includegraphics[width=6.1in]{files/plots/bar-graph-criteria-ready-to-run.pdf}
  \vspace*{-5ex}
  \caption{Responses to the question ``In general, how important are the following characteristics when you are searching for ready-to-run software for a task?''  All \totalrespondents respondents answered the question, but not all respondents chose to select an option for every possible characteristic.  The bar graph is sorted by the sum of the number of times the options ``Essential'' and ``Usually of above-average importance'' were selected for each characteristic.}
  \label{criteria-ready-to-run}
\end{figure}

Unsurprisingly, people reported that the primary search criterion is the availability of specific features (95\%) in the software. Support for specific data standards and file formats (82\%) and software pricing (81\%) are also major considerations, which reflects the culture of scientific computing where software often is expected to be free and specific areas of science often use a limited set of data formats (e.g, FITS in astronomy). Platform requirements, in terms of both operating system (69\%) and hardware (47\%), score more highly than either ease of usage (50\%), installation (40\%) or any performance metrics (39\%). How the software was actually implemented in terms of programming language (23\%) or a particular software architecture (14\%) are unimportant, though. This may be related to the dominance of particular computing configurations within specific sciences, e.g., astronomical computing happens predominantly on Apple or Linux systems where biological computing tends more towards the use of Windows-based systems. A more pressing constraint on any piece of software will then be what it can run on rather than how it was written.

Quality aspects of the software are definitely secondary considerations as far as any software search is concerned with documentation (48\%) rating more highly than either the reputation of the developer (27\%) or the level of software support (18\%). This would tend to suggest that once the software is installed, users expect that they can find in-house workarounds for any bugs, and that they are less concerned about software updates. This illustrates how scientific computing is different from both commercial computing, where software licenses are purchased with implied support, or mobile computing, where software updates are automatically pushed out to devices. 

A more surprising result is that other people's opinions of the software (23\%) and its similarity to other software (12\%) are not important search criteria when they are so important as search mechanisms (see above). It therefore seems likely that a search for software is only necessary when there are no word-of-mouth recommendations or literature recipes for particular tasks. In those cases, the approval rating of the software or its apparent familiarity is largely ignored in favor of other criteria.

Lastly we should note that even though the question explicitly concerned ready-to-use software, the availability of the source code (46\%) and its licensing (41\%) are still relatively important considerations. This is probably a survey bias given the large fraction of respondees who identified as developers (to some degree). Such users are more likely to be willing to---and capable of---altering the software to meet their specific requirements, and the search criteria address this need.


\subsubsection{Search case histories}

We sought examples of respondents' past experiences with the question ``(Optional) Please describe a past scenario when you looked for ready-to-run software.''  We received a total of 23 answers. \fig{sample-responses} provides a sample of 14 out of 23 responses that provided substantial details about the procedures or steps followed.

\begin{figure}[b]
  \small
  \centering
  \begin{minipage}{5.5in}
    \begin{center}
      --- Sample A ---
    \end{center}
    "Looked for a free UML modeler\\
    Googling for some sofware comparison pages\\
    Tried a few one (free or with demo license)\\
    Kept ArgoULM\\
    Not really happy with it
    "
  \end{minipage}\\
  \vspace*{2ex}
  \begin{minipage}{5.5in}
    \begin{center}
      --- Sample B ---
    \end{center}
"Recently I needed to find a package that would let me generate uuids in a specific language.  I wanted this to be simple (it wasn't the main point of the project, so I didn't want to reinvent-the-wheel), it ideally needed to be cross platform (testing on mac, running at scale on linux), and it needed to be something that I could install relatively quickly.\\

One large constraint was I needed to find a package that worked even on relatively out-of-date systems. This meant the newer system-packaged libraries weren't available, and trying to build them from source wasn't all that tractable (I quickly was going down a rabbit hole of other dependencies which weren't available on this old system).  So it was very difficult to find a relatively self-contained, especially when I didn't have root access.\\

My approach was to first check with package managers.  That wasn't terribly helpful, without having root access.  My next attempt was to find what would have been included on a new system (found through Googling, StackOverflow posts, man pages, etc). Then finally, I had to start searching for older versions, which were more standalone.  This final process of searching for older packages was much more random-walk googling + trial-and-error.  This last stage was probably the biggest pain.\\

(Ultimately I was able to find some software that did what I needed.)"  \end{minipage}\\
  \vspace*{2ex}
  \begin{minipage}{5.5in}
    \begin{center}
      --- Sample C ---
    \end{center}
"Looking for an authorization management software (not authentication).\\
I did some online search, plus got some info from a conference and colleagues developing one.\\
The search went on by some specific requirement about the authorization data model used by software and the interfaces available to the authorization data base.\\
Still no exact solution found.\\
One software was too complex with respect with the tasks we need and seemed to miss a top requirement (still investigating).\\
Another seems simpler but definitely lacks documentation on how to start using it."
  \end{minipage}
  \caption{Samples of responses received to the question ``(Optional) Please describe a past scenario when you looked for ready-to-run software.''  A total of 23 survey respondents answered this question.}
  \label{sample-responses}
\end{figure}  

Analysis of the responses showed that the tasks covered a wide range of applications, ranging from authorization management software to MCMC samplers to visualization packages. However, the actual process described by different people was often essentially the same: firstly a broad search, usually using Google, with three to four keywords relating to functionality, implementation, and particular formats or standards, if required: e.g. "fits viewer windows" or "grib format Linux". Recommendations from colleagues can also replace the initial broad search phase. The results were reviewed on the basis of the brief descriptions returned and if one obviously met the search context then the link was followed. A typical search would follow four to five such links and subsequent review criteria were then used to compare these. More recent software is definitely favored but many final decisions are only made when different packages have been installed and compared at a functional or operational level: "I like to develop my own critical view by testing extensively the software".

This anecdotal evidence is consistent with the responses from specific questions. Online searches (for ready-to-use software) are primarily used to identify an initial set of candidate packages that meet a particular set of broad criteria for subsequent (offline) evaluation rather than resulting in the trusted identification of a specific match to a sophisticated query. Although this is a reflection of user bias, it also points to a lack of functionality in general search engines: one respondent wrote "I really miss [\emph{a specific software index}] because it allowed you to search for software with particular tags/keywords with particular license and language requirements. I haven't seen a good replacement for that."

Interestingly, the reputation of the organization producing the software can result in a level of trust in the search results: "Saw the first match ... [and] trusted it because their employees and contractors are a large portion of the folks worldwide who work with that data format." The final match also does not have to be a perfect solution, as indicated by comments made in the case descriptions such as "not really happy with it" to "works well enough". This suggests a degree of pragmatism that workarounds will be found for minor issues with the identified software (see above).


\subsection{Source code}

We now the consider the search for software source code.  As described in \sec{demographics}, the survey included the question ``Are you involved in software development?'' Responses to this question were used to control whether the survey system showed additional questions related to software development.  One of those questions was ``How often do you search online for software source code?'' with six predefined answer options that included ``Never''.  If respondents chose any option other than ``Never'', they were shown additional questions related to searching for source code.  In this section, we discuss the results of this part of the survey.


\subsubsection{Motivations}

Out of the overall \totalrespondents respondents, \totalsearchers (80\%) of them indicated they searched for source code at least some of the time.  \fig{how-often-search-for-src} summarizes the responses received to the question ``How often do you search online for software source code?''

\begin{figure}[bht]
  \centering
  \includegraphics[scale=0.9]{files/plots/bar-graph-how-often-src.pdf}
  \vspace*{-2ex}
  \caption{Responses to the question  ``How often do you search online for software source code?''  Answer choices were presented as the mutually-exclusive multiple choices shown on the vertical axis.}
  \label{how-often-search-for-src}
\end{figure}

We also sought to understand the motivations for why people search for software with the question ``What are some reasons why you look for source code (when you do look)?''  Similar to other questions in the survey, it included multiple non-exclusive choices and a free-text ``Other'' field as the answer options.  \fig{why-search-for-src} summarizes the responses to this question.

\begin{figure}[thb]
  \centering
  \includegraphics[width=5.75in]{files/plots/bar-graph-why-search-for-src.pdf}
  \vspace*{-2ex}
  \caption{Responses to the question ``What are some reasons why you look for source code (when you do look)?''  This question offered the first eight predefined categories and an additional slot for free text under ``Other''.  Answer choices were non-exclusive.}
  \label{why-search-for-src}
\end{figure}

The results show that our sample of developers typically search online for source code at least once a month (78\%) with some (21\%) searching at least once a day. Coding by example (76\%) or reusing existing code as-is (72\%) are the strongest motivations for searching. This indicates a clear preference for not reinventing the wheel or, at least, a reluctance to start from an absolutely blank slate when developing. Software refactoring is another common reason for searching, either to find a more efficient approach to an existing task (42\%) or to discover new algorithms and data structures (36\%). The web is also used as an online reference source to recall syntactic details (44\%) or learn unfamiliar concepts (46\%). \note{This is an interesting use case and one we may want to consider further when building interfaces}

Two respondents wrote answers in the ``Other'' field: one wrote ``Find software libraries to use'', and the other, ``To understand in detail behaviour of software I'm using''.  Neither are subsumed exactly by other answer options, and indicate additional uses for software source code search that we had not anticipated.  The former answer suggests that examining other developers' code can lead to the discovery of previously unknown software libraries; this is different from searching for how specific APIs are used because it does not presuppose knowing which API library will be found or used.  The second answer suggests that another use of searching for source code is to understand detailed properties of some software, which is different from seeking to reuse code, learning how to use a specific API, or learning about unfamiliar code concepts.


\subsubsection{Approaches}

General-purpose search engines (91\%) remain the primary mechanism used to search for source code but personal recommendations (54\%) and similar work reported in the scientific literature (45\%) are employed significantly less than when looking for ready-to-use software. This suggests that software development is a more subjective activity and so source code searches have a more personalized signature to them, determined in part by personal coding styles. This may have implications for any search anonymity issues. This is also supported by the two main factors identified for the failure of past source code searches: an inability to find any code suitable for purpose (69\%) and a belief that the search requirements were too unique (65\%).

Online forums remain underutilized (with a mean of 23\%) but there is slightly greater use of public code repositories (46\%), although though not significantly so. Specialized code search engines (19\%) and software indexes (22\%) are also less used than expected but this may again reflect a lack of knowledge of their existence or coverage. It may also be another instance of sample bias with a more mature audience less likely to use or trust newer domain-specific software, preferring more proven general-purpose mechanisms.
24\% cited lack of trust in the options found as the reason for a previous failed search result.

\subsubsection{Failure}

There is a reasonable indication (47\%) that not enough time is available to conduct proper searches for source code or to evaluate the results. General-purpose search engines are likely to find millions of matches to search terms which may make it difficult to find any true search result easily, particularly if the ranking system of the search engine is optimizing a commercial metric, such as potential advertising revenue. It also seems that software licensing (16\%) was a minor hindrance to previous successful source code searches, even though it was a relatively important criterion for ready-to-use software, but was a large factor (49\%) in not actually reusing the source code found. This would suggest that intellectual property information is not sufficiently visible during software searches.

\subsubsection{Result reliability: code reuse}

The quality of the documentation (61\%) and implementation details -- language (61\%) and operating system (51\%) -- are the prime factors in determining whether found software will be reused. An inability to compile (45\%) or install (29\%) the source code also seems to be a relatively common problem, although it is unclear whether this is due to issues with third-party dependencies, compiler versioning, or the actual source code itself. There is also some discrepancy between what code purports to do (and so matches a search) and whether it actually can: this seems to be more common with functionality (39\%) aspects than support for particular standards (16\%). 

The actual quality (33\%) of the code itself, its structure (31\%), and the design of any algorithm (29\%) are less significant factors. Performance (22\%) and verifiability (8\%) are also somewhat surprisingly minor considerations to reuse but this may be related to the didactic nature of many searches: it may not actually be that important if the code is efficient (or even fully functional) if you just want to learn something.

The level of support (24\%) for any code remains a minor factor and pricing (31\%) is also a less significant consideration than for ready-to-run software. Once code has been identified that actually meets the search parameters, it seems that developers are more willing to pay for it (presumably there may be some way to pass the costs on to end-users).

\subsection{Software metadata}

The survey polled what information would be useful to include in a software index or catalog. Simple terms related to functional -- name (88\%), purpose (93\%), domain of application (78\%), data formats supported (76\%) -- or operational -- operating system(s) (91\%), data licensing (78\%), and dependencies (69\%) -- aspects of the software scored most highly. Terms related to an increased degree of sophistication in these areas, such as the types of user interfaces offered (56\%), a programmable API (51\%), or if the installation process made use of common tools (40\%), were ranked less highly. 
The availability of documentation, either online (78\%), in online forums (54\%), or in an associated paper (50\%), was also an relatively important criterion, although the availability of support (49\%) or bug tracking (41\%) continue not to be so. 

Implementation details, such as the availability of source code (62\%), which language(s) (57\%) was used, and the name of the developer (43\%), would be more important to developers. Again, however, more formal aspects -- test cases (28\%), well-commented code (26\%) or metrics evaluating code quality (26\%) -- rank low. It should be noted that the development status of any code (53\%) and when it was last updated (71\%) rate more highly. This suggests that there is a preference for current software rather than legacy or dead projects, even if they match other search requirements.


\subsection{Survey feedback}

Comments (14) on the survey were largely positive with one or two suggestions about the survey style. Many expressed the opinion that the amount of software out there made the construction of a software catalog or index a difficult task but that it would a worthwhile endeavor. A few also commented on the broader issue of searching for software, saying that "although [they] do this quite often, [they] never really considered how or why code search succeed or not", and the survey "caused a bit of thinking about issues I rarely consider." One response was more specific on this point: "I wasn't conscious of having explicitly searched for software for a long time. I had to think about it, but turns out I do, quite a lot - but indirectly. I don't search for software - I search for solutions. I search for information about a specific issue, using a fragment from a stack trace, a specific error code, or two or three key words to describe the issue...An indirect reference, such as a one-liner that says 'use xyz to check the status' is more likely to result in a follow up than a whole article written specifically about a product." 


\section{Related work}

As part of this project, we performed a lengthy literature survey seeking out research on topics relevant to those in our survey.  We especially sought out past work that may have surveyed how people find or discover software.  We discuss our findings in this section.

\subsection{Surveys examining how software users find ready-to-run software}

Though numerous past surveys have examined software developers and search characteristics in the context of software \emph{code} reuse, extremely few have examined how users---whether they are developers or not---go about locating \emph{ready-to-run} software.  Our research uncovered only two reports of surveys that were not focused specifically on a software development context~\cite{joppa2013troubling, huang2013provenance}.

Joppa et al.~\cite{joppa2013troubling} surveyed 596 scientists working in a single domain (species distribution modeling) in 2013, and asked them what software they used and why they chose that particular software.  The reasons given by the respondents provide some insight into how the scientists found the software they used, thus addressing indirectly the same topic underlying our survey question ``When you need to find ready-to-run software for a particular task, how do you go about finding software?''  In order of most popular to least, the answers that mention something about ``how'' were:
\begin{itemize}

\item ``I tried lots of software and this is the best'' (18\% of respondents)
\item ``Recommendation from close colleagues'' (18\%)
\item ``Personal recommendation'' (9\%)
\item ``Other'' (9\%)
\item ``Recommendation through a training course'' (7\%)
\item ``Because of a good presentation and/or paper I saw'' (4\%)
\item ``A reviewer suggested I use it'' (1\%)

\end{itemize}

It is interesting to note that none of the responses in Joppa et al.'s~\cite{joppa2013troubling} survey explicitly mentioned searching the web, although it is possible that some of the answers such as ``I tried lots of software and this is the best'' and ``Other'' subsumed web searches.

A report by Huang et al.~\cite{huang2013provenance} summarizes interviews of 15 researchers working in bioinformatics.  The subjects included students and full-time faculty.  The authors reported four factors influenced the selection of scientific software: (1) suggestions given by mentors or more senior members of an institution to students or more junior members; (2) mentor involvement in the development of the software, in cases where mentors are also developers; (3) the number of publications \emph{about} some software; and (4) the reputation of the software, assessed by counting the number of publications mentioning the \emph{use} of the software.  Huang et al.'s report~\cite{huang2013provenance} does not include any quantitative or qualitative data about the relative importance of these factors.


\subsection{Surveys examining how developers reuse software}

Most studies of how users find software have done so in the context of software development and the reuse of software code.  The types of reuse in these situations range from black-box reuse of libraries or other software components (i.e., reusing code ``as-is''), to reuse of code fragments; in addition, in programming contexts, many studies examine the reuse of other kinds of artifacts such as documentation, specifications, architectural patterns, and more.  It is important to keep in mind that the approaches used by developers in these situations will likely be different from those used by end users to find software, both because developers are known to use different strategies~\cite{} and because are likely to be considering different characteristics (e.g., APIs).  Nevertheless, such studies are related to our survey.  For the purposes of this review, we discuss general studies about code reuse in this section, and leave those specifically about code search to the next section.

One of the earliest surveys of software developers in the context of software reuse was published by Frakes and Fox~\cite{frakes1995sixteen}.  In their 1991--1992 survey of 28 U.S. organizations, they received responses to 16 questions from 113 people about questions ranging from programming language and tool preferences to the impact of legal issues.  Several of their questions and results are relevant to the topics of our own survey:
\begin{itemize}

\item On the topic of whether reuse is more common in some industries than others, they found that indeed it is.  The subjects in Frakes and Fox's study~\cite{frakes1995sixteen} mostly came from high-technology sectors such as the software industry, aerospace and telecommunications.  The authors found significant differences in the reuse of artifacts between different industries, as well as the types of artifacts that were reused.  The telecommunications industry had the highest level of reuse and had the aerospace the lowest.

\item On the topic of whether having a repository improved reuse, they found that it did not.  Frakes and Fox reported that ``organizations with a repository have median code reuse levels 10 percent higher than organizations that do not have reuse repositories, but this difference is not statistically significant at the 0.05 level.''

\item The authors also examined whether company, division or project sizes were predictive of systematic reuse in an organization.  They found no significant correlation between reuse levels and sizes.

\item On the topic of whether respondents prefer to develop their own software versus reusing someone else's, Frakes and Fox found that 72\% of respondents did not have a ``not invented here'' mentality---most developers prefer to reuse when possible.

\item Several of the questions concerned factors that may or may not affect reuse behavior.  We can summarize their results as follows: CASE (Computer Aided Software Engineering) tools do not promote reuse; education about reuse practices improves the level of reuse in an organization; reuse is higher in organizations having a process that promotes reuse;  recognition for practicing reuse does not increase reuse by individuals, but monetary rewards do; and satisfaction with the quality of reusable artifacts did not affect reuse levels among the respondents (but this was because the level of quality they encountered had been adequate).

\end{itemize}

Samadi et al.~\cite{samadi_2004} report preliminary findings from a 2004 survey conducted by the NASA Earth Science Software Reuse Working Group. They survey was distributed to government employees and contractors in the Earth science community, and asked about people's recent reuse experiences and community needs.  Several results from the study are pertinent to our work:
\begin{itemize}

\item On the topic of how people found reusable software artifacts, the following approaches were noted: (1) word of mouth or personal experiences from past projects, (2) general web search engines (e.g., Google), (3) catalogs and repositories.  The authors report ``Generic search tools (such as Google) were rated as somewhat important, whereas specialist reuse catalogs or repositories were not cited as being particularly important''.

\item On the topic of the criteria used by people to decide which specific components to choose, the authors report that ``most respondents chose saving time/money and ensuring reliability as their primary drivers for reuse''.  Further, the following additional considerations were noted: (1) ``ease of adaption/integration'', (2) availability of source code'', (3) ``cost of creating/acquiring alternative'', and (4) ``recommendation from a colleague''.  The authors further report that (a) availability of support, (b) standards compliance, and (c) testing/certification, were ``not ranked as particularly important''.

\item On the topic of barriers to reuse, two common types of barriers emerged: (1) if available software did not meet a person's specific requirements, and (2) if a given software artifact was ``difficult to understand or poorly documented''.

\end{itemize}

The same study was reprised in 2005 with a wider audience that included members of academia.  The survey and the results of 100 responses they received are summarized by Marshall et al.~\cite{marshall2006software}.  There were four groups of questions: background information, recent reuse experiences, reuse development practices, and community needs.  (The actual questions and detailed responses are not provided in the paper.)  According to Marshall et al., the larger 2005 survey produced essentially similar results to the preliminary 2004 survey.  The noted the following:
\begin{itemize}

\item The primary reason given by people for not reusing software from outside of their group was that ``they did not know where to look for reusable artifacts and they did not know suitable artifacts existed at the time''.

\item For respondents who did engage in reuse, ``personal knowledge from past projects and word-of-mouth or networking were the primary ways of locating and acquiring software development artifacts.''  

\item On the topic of how people located software, the results reported by Marshall et al.~\cite{marshall2006software} seem to be inconsistent.  The authors noted that ``Web searches were of average importance while serendipity and reuse catalogs or repositories were rated the lowest''; however, in another section of the survey dealing with how to increase reuse within the Earth science community, one of the top three factors were ``having an Earth science catalog/repository for reusable artifacts.''  In other words, catalogs were rated low in one part of the survey but high in another part.  Despite this, among their conclusions, Marshall et al. noted ``the use of reuse catalogs and repositories was rated the most important method of increasing the level of reuse within the community.''

\end{itemize}

In a different NASA-centered study, Orrego and Mundy~\cite{orrego_2007_study} studied software reuse in the context of flight control systems at NASA's Independent Verification \& Validation (IV\&V) Facility.  They studied 63 projects using interviews, surveys and case studies.  In interviews with 15 people, they found that black-box reuse and other types of reuse did occur, and the degree to which a given project reused software ranged from 0\% to 80\%.  The difficulty of assessing the characteristics of software was stated as the most problematic aspect of reusing software, usually because of inadequate documentation for the software components to reused.  Unfortunately, the Orrego and Mundy did not report the specific approaches attempted by people to locate software.

Singer et al.~\cite{singer2014software} examined how software developers active on GitHub use Twitter.  They conducted an initial exploratory survey with 271 users of GitHub users (270 of who said they develop software) and followed it up with a validation survey involving 1,413 GitHub users (1,412 of whom said they develop software).  Their results have the following relevance to the topic of how people find and choose software:
\begin{itemize}

\item Developers increase their awareness of people, trends and practices by subscribing to Twitter accounts by (a) individual developers as well as project news channels relevant to their work, (b) news services or news curators, (c) ``thought leaders'' or experts in different subject areas.  

\item Developers extend their knowledge of software (including new software tools and components) by asking and answering questions, participating in conversations, and following experts.  This can learn to serendipitous discovery of reusable methods, software components and software tools.  Singer et al. noted ``Some developers mentioned that Twitter helps them nd and learn about things that they would not have been able to search for themselves, such as emerging technolo-gies that are too new to appear in web searches.''

\end{itemize}

Bauer et al.~\cite{bauer2014exploratory} describe a study of reuse practices by developers at Google.  Despite the nature of Google as one of the preeminent software organizations today, it is surprising that there is no centrally-controlled mandate about reuse of software.  The question of what to reuse, and how, is left to engineers and managers.  (There are core libraries and software components that get reused company-wide; these are under the care of dedicated teams, but the decision of what to use for a given task or application is evidently up to individuals and product teams.)  Several of the questions in Bauer et al.'s survey~\cite{bauer2014exploratory} are relevant to the present study.
\begin{itemize}

\item In one question, they asked subjects for their top three ways of sharing software components.  They received 63 responses: common repository (97\%), packaged libraries (34\%), tutorials (31\%), blogs (19\%), email (9\%), ``I do not share artefacts'' (3\%), and ``other'' (3\%).

\item In another question, they asked about the preferred ways to find reusables.  They received 106 responses: code search (77\%), communication with colleagues (64\%), web search (49\%), browsing repositories (41\%), browsing documentation (23\%), ``other'' (8\%), ``code completion'' (5\%), code recommendation systems (3\%), and tutorials (3\%).  It is worth noting that Google has a centralized code repository where most of the code is available for all projects; this system features a centralized code search facility.

\item Another question, ``What do you do to properly understand and adequately select reusable artefacts'' yielded 115 responses: interface documentation (72\%), examples of usage on blogs and tutorials (64\%), reviewing implementations (64\%), reading guidelines (51\%), exploring third-party products (28\%), ``other'' (10\%), and participating in training for third-party products (5\%).

\end{itemize}


\subsection{Surveys examining code search by developers}

A number of other studies have examined how developers use search to discover software.  While obviously relevant to the general question of how users find software, these studies presuppose an answer: the users are performing computer-based search, and not (say) reading papers or asking colleagues for recommendations.  Thus, they have a narrower scope than our survey both in terms of the population studied and in terms of approaches to finding software; on the other hand, they can delve more deeply into the details of how, where, when, and why the subjects searched for software.

Umarji et al.~\cite{umarji_2008, umarji_2013} surveyed Java programmers in 2006--2007 to understand how and why they searched for source code.  They solicited participation to fill out a web survey via invitations to mailing lists and newsgroups, and received 69 responses.  In the 2008 paper and the 2013 book chapter, they focused on one of the survey questions asking people to describe 1--2 scenarios in which they looked for source code on the Internet.  (A similar but earlier study by Sim et al.~\cite{sim_1998} predated the common use of Internet search for code; it has less relevance to the present work, so we do not report on it here.)  Several facets of the Umarji et al. study are especially relevant to our own survey:
\begin{itemize}

\item With respect to why developers searched, out of a total of 51 searchers described, Umarji et al.~\cite{umarji_2008} found that the largest fraction were concerned with finding either (a) reusable code (67\% of the 51 results), (b) reference examples (33\%), or debugging activities (10\%).  Within the reuse category (a), the authors identified four themes: (1) search for code fragments; (2) search for subsystems that implemented reusable data structures, algorithms, or other elements that could be incorporated into an implementation; (3) search for packages or API libraries; and (4) search for stand-alone tools or systems (in the words of a respondent, ``big piece of code that does more or less what I want'').  Within the category of reference examples (b), Umarji at al. also identified four (different) themes: (1) search for code fragments that illustrate syntax; (2) search for implementations of data structure, algorithm or widgets in order to verify a programmer's own approach or use as a basis for reimplementation; (3) search for examples of how to use a library; and (4) search for similar software to generate new ideas.  Within the category of debugging (c), developers searched for solutions to software defects (``patches'') or explanations for the cause of an error.

\item Umarji and Sim~\cite{umarji_2013} report that common starting points for searches were: (1) recommendations from friends, and (2) reviews, articles, blogs and social tagging sites.

\item With respect to how developers conducted searches, the participants in the survey used the following, in order of popularity: (1) general-purpose search engines (87\% of participants), (2) personal domain knowledge (54\%), (3) project hosting sites such as SourceForge.net~\cite{} (49\%), (4) references from peers (43\%), (4) mailing lists (23\%), and (5) code-specific search engines such as Google Code Search~\cite{} (16\%).

\item With respect to the selection criteria used by developers to choose a solution, Umarji and Sim~\cite{umarji_2013} report that the most important factors in order of importance were: (1) functionality (78\%), (2) type of software license (43\%), (3) price (38\%), (4) amount of user support available (30\%), and (5) level of project activity (26\%).

\end{itemize}

Gallardo-Valencia and Sim~\cite{gallardo2011kinds} examined the web search behaviors of 25 developers working at a software company that develops transactional software for banks and other organizations.  The authors used several methods to gather their data: they asked developers to self-report their activities using record sheets, they performed interviews, and they performed observation of developer activities during their work days.  They reported that 8\% of the web searches performed by developers during the study period were to download libraries or other tools, and another 5\% of the web searches were to collect information to help judge the suitability of software components to be used in implementations.  The rest of the searches concerned finding information about how to do specific tasks or write specific kinds of programs, and debugging problems involving errors in software.

Sim et al.~\cite{sim2012software} performed a pair of surveys as part of an effort to understanding developers' approaches to code search.  One survey was exploratory and used to inform a second, quantitative survey.  They report only a small part of the survey results, specifically concerning the nature of the searches performed in the different surveys.  For purposes of the current review, the quantitative survey results are more relevant.  On the topic of what subjects searched for, 92\% of respondents in their survey answered they had searched for code snippets on past occasions, and 69\% said they had searched for software components.  In terms of motivations for why they conducted searches, 96\% of respondents said they had sought reference examples on past occasions, and 35\% said that they had searched for code to reuse as-is.


\section{Conclusions}

Although the survey response was not large\note{how many would we consider large? There are 2000 professional astronomers in the US and we got ~1\%}, it should be considered broadly representative. The breadth and maturity of response has helped to identify a number of current community practices in science in searching for both ready-to-use software and source code. It has also raised a number of issues that need to be considered in the design of a software index or catalog:

\begin{enumerate}
\item Searching for reference material vs. production code - pedagogic code may not be efficient.
\item Personalized search profiles
\item Result trust issues
\item Evaluatory statistics 
\item Licensing metadata
\item Full-text index matching from READMEs against search strings to identify "one-liners" rather than just keywords
\end{enumerate}


% ......................................................................
\appendix
\section{The Survey}
\label{apdx:survey}

We implemented the survey using Google Forms~\cite{googleforms}, a free online system for creating interactive, Web-based forms.  The following pages show screen captures of the survey as it appeared to users.  Not shown is the switching logic that allowed some parts of the survey to be showed only if users responded in certain ways to some of the questions.  The following are the rules implemented in the switching logic:
\begin{description}

\item \emph{Rule for Question 6}: If the user answers ``I never get to choose the software I use'', the next page shown is the final page of the survey (containing questions~21 and~22).

\item  \emph{Rule for Question 11}: If the user answers ``No'', the next page shown is the final page of the survey.

\item \emph{Rule for Question 16}:  If the user answers ``Never'', the next page shown is the final page of the survey.

\end{description}


\clearpage
\includepdf[trim=0.25in 0.25in 0.25in 0.3in,pages={-}]{files/survey-copy/survey-2015-09-02.pdf}



% ......................................................................
\clearpage
\bibliographystyle{plain}
\bibliography{mike-casics-library}

\end{document}